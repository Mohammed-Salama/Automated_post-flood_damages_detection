{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "CLASSES = [\"flooded\", \"non-flooded\"]\n",
    "def load_data(data_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for c in CLASSES:\n",
    "        path = os.path.join(data_path, c)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img))\n",
    "            img_array = cv2.resize(img_array, (227, 227))\n",
    "            data.append(img_array)\n",
    "            labels.append(CLASSES.index(c))\n",
    "\n",
    "    labels = to_categorical(labels)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# Load data\n",
    "data, labels = load_data(\"./Dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def data_augmentation(data, labels):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    seqs = []\n",
    "    # Define the augmentation sequences\n",
    "    for i in range(1):\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Fliplr(p=random.uniform(0, 1)),\n",
    "            iaa.Crop(percent=(0, random.uniform(0, 0.1))),\n",
    "            iaa.GaussianBlur(sigma=random.uniform(0, 3.0)),\n",
    "            iaa.AdditiveGaussianNoise(scale=(0, random.uniform(0, 0.1*255))),\n",
    "            iaa.Multiply((random.uniform(0.5, 1.5), random.uniform(0.5, 1.5))),\n",
    "            iaa.Affine(\n",
    "                scale={\"x\": (random.uniform(0.8, 1.2), random.uniform(0.8, 1.2)), \"y\": (random.uniform(0.8, 1.2), random.uniform(0.8, 1.2))},\n",
    "                translate_percent={\"x\": (random.uniform(-0.2, 0.2), random.uniform(-0.2, 0.2)), \"y\": (random.uniform(-0.2, 0.2), random.uniform(-0.2, 0.2))},\n",
    "                rotate=(random.uniform(-45, 45), random.uniform(-45, 45)),\n",
    "                shear=(random.uniform(-16, 16), random.uniform(-16, 16))\n",
    "            )\n",
    "        ], random_order=True)\n",
    "        seqs.append(seq)\n",
    "\n",
    "    for i, image in enumerate(data):\n",
    "        augmented_images.append(image)\n",
    "        augmented_labels.append(labels[i])\n",
    "        for seq in seqs:\n",
    "            augmented_image = seq(image=image)\n",
    "            augmented_images.append(augmented_image)\n",
    "            augmented_labels.append(labels[i])\n",
    "    \n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply augmentation on training data\n",
    "X_train_aug, y_train = data_augmentation(X_train, y_train)\n",
    "\n",
    "# apply augmentation on validation data\n",
    "X_val_aug, y_val = data_augmentation(X_val, y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_aug = np.array([extract_image_features(image) for image in X_train_aug])\n",
    "# X_val_aug = np.array([extract_image_features(image) for image in X_val_aug])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    AveragePooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Activation,\n",
    "    MaxPool2D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # 1st layer (CONV + pool + batchnorm)\n",
    "# model.add(\n",
    "#     Conv2D(\n",
    "#         filters=96,\n",
    "#         kernel_size=(11, 11),\n",
    "#         strides=(4, 4),\n",
    "#         padding=\"valid\",\n",
    "#         input_shape=(227, 227, 3),\n",
    "#     )\n",
    "# )\n",
    "# model.add(Activation(\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "# model.add(BatchNormalization())\n",
    "# # 2nd layer (CONV + pool + batchnorm)\n",
    "# model.add(\n",
    "#     Conv2D(\n",
    "#         filters=256,\n",
    "#         kernel_size=(5, 5),\n",
    "#         strides=(1, 1),\n",
    "#         padding=\"same\",\n",
    "#         kernel_regularizer=l2(0.0005),\n",
    "#     )\n",
    "# )\n",
    "# model.add(Activation(\"relu\"))\n",
    "# model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding=\"valid\"))\n",
    "# model.add(BatchNormalization())\n",
    "# # layer 3 (CONV + batchnorm)\n",
    "# model.add(\n",
    "#     Conv2D(\n",
    "#         filters=384,\n",
    "#         kernel_size=(3, 3),\n",
    "#         strides=(1, 1),\n",
    "#         padding=\"same\",\n",
    "#         kernel_regularizer=l2(0.0005),\n",
    "#     )\n",
    "# )\n",
    "# model.add(Activation(\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# # layer 4 (CONV + batchnorm)\n",
    "# model.add(\n",
    "#     Conv2D(\n",
    "#         filters=384,\n",
    "#         kernel_size=(3, 3),\n",
    "#         strides=(1, 1),\n",
    "#         padding=\"same\",\n",
    "#         kernel_regularizer=l2(0.0005),\n",
    "#     )\n",
    "# )\n",
    "# model.add(Activation(\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# # layer 5 (CONV + batchnorm)\n",
    "# model.add(\n",
    "#     Conv2D(\n",
    "#         filters=256,\n",
    "#         kernel_size=(3, 3),\n",
    "#         strides=(1, 1),\n",
    "#         padding=\"same\",\n",
    "#         kernel_regularizer=l2(0.0005),\n",
    "#     )\n",
    "# )\n",
    "# model.add(Activation(\"relu\"))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding=\"valid\"))\n",
    "# model.add(Flatten())\n",
    "# # layer 6 (Dense layer + dropout)\n",
    "# model.add(Dense(units=4096, activation=\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# # layer 7 (Dense layers)\n",
    "# model.add(Dense(units=4096, activation=\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# # layer 8 (softmax output layer)\n",
    "# model.add(Dense(units=2, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# 1st layer (CONV + pool + batchnorm)\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        input_shape=(227, 227, 3),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "# 2nd layer (CONV + pool + batchnorm)\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        kernel_regularizer=l2(0.0005),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding=\"valid\"))\n",
    "model.add(BatchNormalization())\n",
    "# layer 3 (CONV + batchnorm)\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        kernel_regularizer=l2(0.0005),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "# layer 4 (CONV + batchnorm)\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        kernel_regularizer=l2(0.0005),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "# layer 5 (CONV + batchnorm)\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        kernel_regularizer=l2(0.0005),\n",
    "    )\n",
    ")\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding=\"valid\"))\n",
    "model.add(Flatten())\n",
    "# layer 6 (Dense layer + dropout)\n",
    "model.add(Dense(units=1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "# layer 7 (Dense layers)\n",
    "model.add(Dense(units=1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "# layer 8 (softmax output layer)\n",
    "model.add(Dense(units=2, activation=\"softmax\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - 22s 1s/step - loss: 4.4940 - accuracy: 0.6621 - val_loss: 58.9291 - val_accuracy: 0.4703\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 22s 1s/step - loss: 2.4148 - accuracy: 0.7609 - val_loss: 21.9336 - val_accuracy: 0.4703\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 21s 1s/step - loss: 1.4445 - accuracy: 0.7888 - val_loss: 5.4349 - val_accuracy: 0.4703\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 19s 910ms/step - loss: 1.2851 - accuracy: 0.8017 - val_loss: 3.3825 - val_accuracy: 0.4649\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 19s 897ms/step - loss: 1.0168 - accuracy: 0.8469 - val_loss: 2.3076 - val_accuracy: 0.5135\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 21s 1s/step - loss: 0.7028 - accuracy: 0.8741 - val_loss: 0.9528 - val_accuracy: 0.7027\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 20s 955ms/step - loss: 0.6585 - accuracy: 0.8771 - val_loss: 1.0622 - val_accuracy: 0.6622\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 19s 924ms/step - loss: 0.6387 - accuracy: 0.8786 - val_loss: 0.7582 - val_accuracy: 0.8108\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 20s 947ms/step - loss: 0.5199 - accuracy: 0.8989 - val_loss: 0.9115 - val_accuracy: 0.7514\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 19s 923ms/step - loss: 0.6243 - accuracy: 0.8824 - val_loss: 0.6109 - val_accuracy: 0.8270\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 19s 912ms/step - loss: 0.4223 - accuracy: 0.9155 - val_loss: 1.0048 - val_accuracy: 0.7432\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 20s 933ms/step - loss: 0.4414 - accuracy: 0.9163 - val_loss: 1.0816 - val_accuracy: 0.7757\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 21s 1s/step - loss: 0.3710 - accuracy: 0.9306 - val_loss: 0.8585 - val_accuracy: 0.7703\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 20s 972ms/step - loss: 0.3287 - accuracy: 0.9480 - val_loss: 0.6329 - val_accuracy: 0.8486\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 20s 932ms/step - loss: 0.3168 - accuracy: 0.9480 - val_loss: 1.0345 - val_accuracy: 0.7568\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 20s 970ms/step - loss: 0.2780 - accuracy: 0.9548 - val_loss: 2.3043 - val_accuracy: 0.6459\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 20s 952ms/step - loss: 0.2933 - accuracy: 0.9555 - val_loss: 0.6114 - val_accuracy: 0.8541\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 21s 986ms/step - loss: 0.3234 - accuracy: 0.9532 - val_loss: 0.7489 - val_accuracy: 0.8135\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 21s 992ms/step - loss: 0.2977 - accuracy: 0.9548 - val_loss: 0.8419 - val_accuracy: 0.8514\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 20s 950ms/step - loss: 0.2765 - accuracy: 0.9548 - val_loss: 0.7320 - val_accuracy: 0.8838\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 20s 976ms/step - loss: 0.2918 - accuracy: 0.9638 - val_loss: 0.8877 - val_accuracy: 0.8081\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 19s 904ms/step - loss: 0.2593 - accuracy: 0.9721 - val_loss: 0.7979 - val_accuracy: 0.8162\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 20s 939ms/step - loss: 0.2855 - accuracy: 0.9646 - val_loss: 0.6758 - val_accuracy: 0.8703\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 20s 942ms/step - loss: 0.2923 - accuracy: 0.9608 - val_loss: 1.4931 - val_accuracy: 0.7027\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 19s 904ms/step - loss: 0.2734 - accuracy: 0.9676 - val_loss: 1.0954 - val_accuracy: 0.7811\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 19s 924ms/step - loss: 0.2460 - accuracy: 0.9721 - val_loss: 0.8043 - val_accuracy: 0.8730\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 19s 931ms/step - loss: 0.2599 - accuracy: 0.9751 - val_loss: 0.8519 - val_accuracy: 0.8595\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 19s 915ms/step - loss: 0.2264 - accuracy: 0.9766 - val_loss: 0.8814 - val_accuracy: 0.8216\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 18s 829ms/step - loss: 0.2516 - accuracy: 0.9698 - val_loss: 0.7526 - val_accuracy: 0.8676\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 25s 1s/step - loss: 0.2388 - accuracy: 0.9729 - val_loss: 0.9592 - val_accuracy: 0.8189\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 20s 971ms/step - loss: 0.2398 - accuracy: 0.9781 - val_loss: 0.7936 - val_accuracy: 0.8676\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 20s 942ms/step - loss: 0.2380 - accuracy: 0.9706 - val_loss: 0.8278 - val_accuracy: 0.8324\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 20s 951ms/step - loss: 0.2423 - accuracy: 0.9653 - val_loss: 0.8933 - val_accuracy: 0.8378\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 20s 937ms/step - loss: 0.2299 - accuracy: 0.9729 - val_loss: 1.7420 - val_accuracy: 0.6838\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 19s 895ms/step - loss: 0.1917 - accuracy: 0.9864 - val_loss: 0.7110 - val_accuracy: 0.8649\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 19s 907ms/step - loss: 0.1768 - accuracy: 0.9902 - val_loss: 0.5183 - val_accuracy: 0.8919\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 19s 911ms/step - loss: 0.2237 - accuracy: 0.9781 - val_loss: 1.1036 - val_accuracy: 0.8162\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 21s 985ms/step - loss: 0.2164 - accuracy: 0.9834 - val_loss: 0.9186 - val_accuracy: 0.8297\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 21s 980ms/step - loss: 0.2392 - accuracy: 0.9698 - val_loss: 0.9421 - val_accuracy: 0.8676\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 21s 989ms/step - loss: 0.2589 - accuracy: 0.9781 - val_loss: 1.6715 - val_accuracy: 0.7432\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 21s 1s/step - loss: 0.2450 - accuracy: 0.9736 - val_loss: 0.7006 - val_accuracy: 0.8730\n",
      "Epoch 42/50\n",
      " 1/21 [>.............................] - ETA: 19s - loss: 0.1593 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val_aug, y_val)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ahmed/CMP/Semester 2/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb Cell 18\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/CMP/Semester%202/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_pred, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/CMP/Semester%202/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(y_val, y_pred)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ahmed/CMP/Semester%202/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m cm_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(cm, index\u001b[39m=\u001b[39mcategories, columns\u001b[39m=\u001b[39mcategories)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ahmed/CMP/Semester%202/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m sns\u001b[39m.\u001b[39mheatmap(cm_df, annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cmap\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBlues\u001b[39m\u001b[39m\"\u001b[39m, fmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ahmed/CMP/Semester%202/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mConfusion Matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=CATEGORIES, columns=CATEGORIES)\n",
    "sns.heatmap(cm_df, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Plot f1 score\n",
    "f1 = f1_score(y_val, y_pred, average=None)\n",
    "f1_df = pd.DataFrame(f1, index=CATEGORIES, columns=[\"F1 Score\"])\n",
    "f1_df.plot(kind=\"bar\", ylim=(0, 1))\n",
    "plt.title(\"F1 Score\")\n",
    "plt.show()\n",
    "\n",
    "# describe history for accuracy\n",
    "plt.plot(history.history[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# model = load_model(\"model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1525, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1514, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1507, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1473, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2, 2) and (None, 2) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ahmed/CMP/Semester 2/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb Cell 26\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/CMP/Semester%202/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# test the model on the test data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ahmed/CMP/Semester%202/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(test_data, test_labels, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/CMP/Semester%202/SI/PROJ/Automated_post-flood_damages_detection/main.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTest accuracy:\u001b[39m\u001b[39m\"\u001b[39m, test_acc)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1525, in test_function  *\n        return step_function(self, iterator)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1514, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1507, in run_step  **\n        outputs = model.test_step(data)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1473, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/ahmed/.local/lib/python3.10/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2, 2) and (None, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# test the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
